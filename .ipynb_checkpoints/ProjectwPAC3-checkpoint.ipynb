{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact or Fake: News Analysis\n",
    "\n",
    "## Data Mining 334\n",
    "## Alex Laughlin, Xandre Clementsmith, Terence Carey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Creation and Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in data\n",
    "\n",
    "#read in politifact\n",
    "df_p_real = pd.read_csv('politifact_real.csv')\n",
    "df_p_fake = pd.read_csv('politifact_fake.csv')\n",
    "\n",
    "df_p_real['true/false'] = True\n",
    "\n",
    "df_p_fake['true/false'] = False\n",
    "\n",
    "#read in gossipcop data\n",
    "df_g_real = pd.read_csv('gossipcop_real.csv')\n",
    "df_g_fake = pd.read_csv('gossipcop_fake.csv')\n",
    "\n",
    "df_g_real['true/false'] = True\n",
    "\n",
    "df_g_fake['true/false'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1056, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Join tables together\n",
    "sets = \"p\"\n",
    "if sets == \"p\":\n",
    "    frames = [df_p_real, df_p_fake]\n",
    "elif sets == \"g\":\n",
    "    frames = [df_g_real, df_g_fake]\n",
    "elif sets == \"b\":\n",
    "    frames = [df_p_real, df_p_fake, df_g_real, df_g_fake]\n",
    "df = pd.concat(frames)\n",
    "df.head(5)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>news_url</th>\n",
       "      <th>title</th>\n",
       "      <th>tweet_ids</th>\n",
       "      <th>true/false</th>\n",
       "      <th>parsedURL</th>\n",
       "      <th>extension</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>tokens_without_stopwords</th>\n",
       "      <th>tokens_stemmed</th>\n",
       "      <th>tokens_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>politifact14984</td>\n",
       "      <td>http://www.nfib-sbet.org/</td>\n",
       "      <td>National Federation of Independent Business</td>\n",
       "      <td>967132259869487105\\t967164368768196609\\t967215...</td>\n",
       "      <td>True</td>\n",
       "      <td>www.nfib-sbet.org</td>\n",
       "      <td>org</td>\n",
       "      <td>[National, Federation, Independent, Business]</td>\n",
       "      <td>[National, Federation, Independent, Business]</td>\n",
       "      <td>[nation, feder, independ, busi]</td>\n",
       "      <td>[National, Federation, Independent, Business]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>politifact12944</td>\n",
       "      <td>http://www.cq.com/doc/newsmakertranscripts-494...</td>\n",
       "      <td>comments in Fayetteville NC</td>\n",
       "      <td>942953459\\t8980098198\\t16253717352\\t1668513250...</td>\n",
       "      <td>True</td>\n",
       "      <td>www.cq.com</td>\n",
       "      <td>com</td>\n",
       "      <td>[comments, Fayetteville]</td>\n",
       "      <td>[comments, Fayetteville]</td>\n",
       "      <td>[comment, fayettevill]</td>\n",
       "      <td>[comment, Fayetteville]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>politifact333</td>\n",
       "      <td>https://web.archive.org/web/20080204072132/htt...</td>\n",
       "      <td>Romney makes pitch, hoping to close deal : Ele...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>web.archive.org</td>\n",
       "      <td>org</td>\n",
       "      <td>[Romney, makes, pitch, hoping, close, deal, El...</td>\n",
       "      <td>[Romney, makes, pitch, hoping, close, deal, El...</td>\n",
       "      <td>[romney, make, pitch, hope, close, deal, elect...</td>\n",
       "      <td>[Romney, make, pitch, hoping, close, deal, Ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>politifact4358</td>\n",
       "      <td>https://web.archive.org/web/20110811143753/htt...</td>\n",
       "      <td>Democratic Leaders Say House Democrats Are Uni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>web.archive.org</td>\n",
       "      <td>org</td>\n",
       "      <td>[Democratic, Leaders, Say, House, Democrats, A...</td>\n",
       "      <td>[Democratic, Leaders, Say, House, Democrats, A...</td>\n",
       "      <td>[democrat, leader, say, hous, democrat, are, u...</td>\n",
       "      <td>[Democratic, Leaders, Say, House, Democrats, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>politifact779</td>\n",
       "      <td>https://web.archive.org/web/20070820164107/htt...</td>\n",
       "      <td>Budget of the United States Government, FY</td>\n",
       "      <td>89804710374154240\\t91270460595109888\\t96039619...</td>\n",
       "      <td>True</td>\n",
       "      <td>web.archive.org</td>\n",
       "      <td>org</td>\n",
       "      <td>[Budget, the, United, States, Government]</td>\n",
       "      <td>[Budget, United, States, Government]</td>\n",
       "      <td>[budget, unit, state, govern]</td>\n",
       "      <td>[Budget, United, States, Government]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>politifact14064</td>\n",
       "      <td>http://www.politifact.com/truth-o-meter/statem...</td>\n",
       "      <td>Donald Trump exaggerates when he says China ha...</td>\n",
       "      <td>690248006399049728\\t690254026663821312\\t690276...</td>\n",
       "      <td>True</td>\n",
       "      <td>www.politifact.com</td>\n",
       "      <td>com</td>\n",
       "      <td>[Donald, Trump, exaggerates, when, says, China...</td>\n",
       "      <td>[Donald, Trump, exaggerates, says, China, tota...</td>\n",
       "      <td>[donald, trump, exagger, say, china, total, co...</td>\n",
       "      <td>[Donald, Trump, exaggerates, say, China, total...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>politifact14474</td>\n",
       "      <td>https://www.law.cornell.edu/constitution/amend...</td>\n",
       "      <td>25th Amendment</td>\n",
       "      <td>1262604762\\t10969740933\\t11182364398\\t17507543...</td>\n",
       "      <td>True</td>\n",
       "      <td>www.law.cornell.edu</td>\n",
       "      <td>edu</td>\n",
       "      <td>[25th, Amendment]</td>\n",
       "      <td>[25th, Amendment]</td>\n",
       "      <td>[25th, amend]</td>\n",
       "      <td>[25th, Amendment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>politifact5276</td>\n",
       "      <td>http://americaneedsmitt.com/blog/2011/11/10/mi...</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>americaneedsmitt.com</td>\n",
       "      <td>com</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>politifact1313</td>\n",
       "      <td>https://web.archive.org/web/20090913221204/htt...</td>\n",
       "      <td>Briefing by White House Press Secretary Robert...</td>\n",
       "      <td>13511762265\\t13512918230\\t13513835900\\t1351424...</td>\n",
       "      <td>True</td>\n",
       "      <td>web.archive.org</td>\n",
       "      <td>org</td>\n",
       "      <td>[Briefing, White, House, Press, Secretary, Rob...</td>\n",
       "      <td>[Briefing, White, House, Press, Secretary, Rob...</td>\n",
       "      <td>[brief, white, hous, press, secretari, robert,...</td>\n",
       "      <td>[Briefing, White, House, Press, Secretary, Rob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>politifact937</td>\n",
       "      <td>https://web.archive.org/web/20080623122709/htt...</td>\n",
       "      <td>A Solar Grand Plan: Scientific American</td>\n",
       "      <td>140962137332920320\\t141057766704947200\\t141166...</td>\n",
       "      <td>True</td>\n",
       "      <td>web.archive.org</td>\n",
       "      <td>org</td>\n",
       "      <td>[Solar, Grand, Plan, Scientific, American]</td>\n",
       "      <td>[Solar, Grand, Plan, Scientific, American]</td>\n",
       "      <td>[solar, grand, plan, scientif, american]</td>\n",
       "      <td>[Solar, Grand, Plan, Scientific, American]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                           news_url  \\\n",
       "0  politifact14984                          http://www.nfib-sbet.org/   \n",
       "1  politifact12944  http://www.cq.com/doc/newsmakertranscripts-494...   \n",
       "2    politifact333  https://web.archive.org/web/20080204072132/htt...   \n",
       "3   politifact4358  https://web.archive.org/web/20110811143753/htt...   \n",
       "4    politifact779  https://web.archive.org/web/20070820164107/htt...   \n",
       "5  politifact14064  http://www.politifact.com/truth-o-meter/statem...   \n",
       "6  politifact14474  https://www.law.cornell.edu/constitution/amend...   \n",
       "7   politifact5276  http://americaneedsmitt.com/blog/2011/11/10/mi...   \n",
       "8   politifact1313  https://web.archive.org/web/20090913221204/htt...   \n",
       "9    politifact937  https://web.archive.org/web/20080623122709/htt...   \n",
       "\n",
       "                                               title  \\\n",
       "0        National Federation of Independent Business   \n",
       "1                        comments in Fayetteville NC   \n",
       "2  Romney makes pitch, hoping to close deal : Ele...   \n",
       "3  Democratic Leaders Say House Democrats Are Uni...   \n",
       "4        Budget of the United States Government, FY    \n",
       "5  Donald Trump exaggerates when he says China ha...   \n",
       "6                                     25th Amendment   \n",
       "7                                                      \n",
       "8  Briefing by White House Press Secretary Robert...   \n",
       "9            A Solar Grand Plan: Scientific American   \n",
       "\n",
       "                                           tweet_ids  true/false  \\\n",
       "0  967132259869487105\\t967164368768196609\\t967215...        True   \n",
       "1  942953459\\t8980098198\\t16253717352\\t1668513250...        True   \n",
       "2                                                NaN        True   \n",
       "3                                                NaN        True   \n",
       "4  89804710374154240\\t91270460595109888\\t96039619...        True   \n",
       "5  690248006399049728\\t690254026663821312\\t690276...        True   \n",
       "6  1262604762\\t10969740933\\t11182364398\\t17507543...        True   \n",
       "7                                                NaN        True   \n",
       "8  13511762265\\t13512918230\\t13513835900\\t1351424...        True   \n",
       "9  140962137332920320\\t141057766704947200\\t141166...        True   \n",
       "\n",
       "              parsedURL extension  \\\n",
       "0     www.nfib-sbet.org       org   \n",
       "1            www.cq.com       com   \n",
       "2       web.archive.org       org   \n",
       "3       web.archive.org       org   \n",
       "4       web.archive.org       org   \n",
       "5    www.politifact.com       com   \n",
       "6   www.law.cornell.edu       edu   \n",
       "7  americaneedsmitt.com       com   \n",
       "8       web.archive.org       org   \n",
       "9       web.archive.org       org   \n",
       "\n",
       "                                     tokenized_sents  \\\n",
       "0      [National, Federation, Independent, Business]   \n",
       "1                           [comments, Fayetteville]   \n",
       "2  [Romney, makes, pitch, hoping, close, deal, El...   \n",
       "3  [Democratic, Leaders, Say, House, Democrats, A...   \n",
       "4          [Budget, the, United, States, Government]   \n",
       "5  [Donald, Trump, exaggerates, when, says, China...   \n",
       "6                                  [25th, Amendment]   \n",
       "7                                                 []   \n",
       "8  [Briefing, White, House, Press, Secretary, Rob...   \n",
       "9         [Solar, Grand, Plan, Scientific, American]   \n",
       "\n",
       "                            tokens_without_stopwords  \\\n",
       "0      [National, Federation, Independent, Business]   \n",
       "1                           [comments, Fayetteville]   \n",
       "2  [Romney, makes, pitch, hoping, close, deal, El...   \n",
       "3  [Democratic, Leaders, Say, House, Democrats, A...   \n",
       "4               [Budget, United, States, Government]   \n",
       "5  [Donald, Trump, exaggerates, says, China, tota...   \n",
       "6                                  [25th, Amendment]   \n",
       "7                                                 []   \n",
       "8  [Briefing, White, House, Press, Secretary, Rob...   \n",
       "9         [Solar, Grand, Plan, Scientific, American]   \n",
       "\n",
       "                                      tokens_stemmed  \\\n",
       "0                    [nation, feder, independ, busi]   \n",
       "1                             [comment, fayettevill]   \n",
       "2  [romney, make, pitch, hope, close, deal, elect...   \n",
       "3  [democrat, leader, say, hous, democrat, are, u...   \n",
       "4                      [budget, unit, state, govern]   \n",
       "5  [donald, trump, exagger, say, china, total, co...   \n",
       "6                                      [25th, amend]   \n",
       "7                                                 []   \n",
       "8  [brief, white, hous, press, secretari, robert,...   \n",
       "9           [solar, grand, plan, scientif, american]   \n",
       "\n",
       "                                   tokens_lemmatized  \n",
       "0      [National, Federation, Independent, Business]  \n",
       "1                            [comment, Fayetteville]  \n",
       "2  [Romney, make, pitch, hoping, close, deal, Ele...  \n",
       "3  [Democratic, Leaders, Say, House, Democrats, A...  \n",
       "4               [Budget, United, States, Government]  \n",
       "5  [Donald, Trump, exaggerates, say, China, total...  \n",
       "6                                  [25th, Amendment]  \n",
       "7                                                 []  \n",
       "8  [Briefing, White, House, Press, Secretary, Rob...  \n",
       "9         [Solar, Grand, Plan, Scientific, American]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w{3,}\")\n",
    "\n",
    "#parse source URL and strip down to base form \n",
    "from urllib.parse import urlparse\n",
    "import urllib as urllib\n",
    "df['parsedURL'] = df.apply(lambda row: urlparse(str(row['news_url'])).netloc, axis=1)\n",
    "df['extension'] = df.apply(lambda row: str(row['parsedURL'])[-3:], axis = 1)\n",
    "\n",
    "#remove special characters\n",
    "df['title'] = df.apply(lambda row: re.sub('[^A-Za-z ]{3,}', '', str(row['title'])), axis =1)\n",
    "\n",
    "#tokenize words from title\n",
    "df['tokenized_sents'] = df.apply(lambda row: tokenizer.tokenize(row['title']), axis=1)\n",
    "\n",
    "#remove stop words from tokenized titles\n",
    "df['tokens_without_stopwords'] = df['tokenized_sents'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "#stem tokenized words without stopwords\n",
    "df['tokens_stemmed']=df['tokens_without_stopwords'].apply(lambda x : [stemmer.stem(y) for y in x])\n",
    "\n",
    "#lemmatize tokenized words without stopwords\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "df['tokens_lemmatized'] = df['tokens_without_stopwords'].apply(lambda x : [lemmatizer.lemmatize(y) for y in x])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary and IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODULAR COMPONENT:\n",
    "# select token_type for machine learning algorithms\n",
    "# tokens_stemmed performs best for logistic regression and decision tree\n",
    "# tokens_lemmatized performs best for random forest and passive aggresive classifier\n",
    "\n",
    "token_type = 'tokens_stemmed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create corpus from lemmatized tokens as numpy array\n",
    "corpus = df[token_type].to_numpy()\n",
    "\n",
    "#create dictionary\n",
    "DF = {}\n",
    "\n",
    "#write unique words to dictionary\n",
    "for i in range(len(corpus)):\n",
    "    tokens = corpus[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "            \n",
    "#get word frequency\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])\n",
    "# print(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [x for x in DF]\n",
    "# print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 SHAPE = (1056, 2517)\n",
      "Index(['10k', '22m', '25th', '27ave', '2kilo', '2nd', '3rd', '3year',\n",
      "       'abandon', 'abc',\n",
      "       ...\n",
      "       'wwii', 'year', 'yearbook', 'yellow', 'york', 'young', 'younger',\n",
      "       'youngest', 'youtub', 'zakaria'],\n",
      "      dtype='object', length=2517)\n",
      "EXTENSIONS SHAPE = (1056, 23)\n",
      "Index(['', '.be', '.co', '.is', '.me', '.pw', '.ru', '.tk', '.uk', '.us',\n",
      "       'com', 'edu', 'ews', 'gov', 'ife', 'ite', 'ive', 'lub', 'mil', 'net',\n",
      "       'nfo', 'one', 'org'],\n",
      "      dtype='object')\n",
      "Index(['', '.be', '.co', '.is', '.me', '.pw', '.ru', '.tk', '.uk', '.us',\n",
      "       'com', 'edu', 'ews', 'gov', 'ife', 'ite', 'ive', 'lub', 'mil', 'net',\n",
      "       'nfo', 'one', 'org'],\n",
      "      dtype='object')\n",
      "<bound method NDFrame.head of         .be  .co  .is  .me  .pw  .ru  .tk  .uk  .us  ...  wwii  year  \\\n",
      "0    0    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "0    1    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "1    0    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "1    1    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "2    0    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
      "619  0    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "620  0    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "621  0    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "622  0    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "623  0    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "\n",
      "     yearbook  yellow  york  young  younger  youngest  youtub  zakaria  \n",
      "0         0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "0         0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "1         0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "1         0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "2         0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "..        ...     ...   ...    ...      ...       ...     ...      ...  \n",
      "619       0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "620       0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "621       0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "622       0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "623       0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "\n",
      "[1056 rows x 2540 columns]>\n"
     ]
    }
   ],
   "source": [
    "#create column of strings from lemmatized tokens\n",
    "df['tfidfprep']=[\" \".join(x) for x in df[token_type].values]\n",
    "labels=df[[\"true/false\"]]\n",
    "\n",
    "#initialize tfidf vectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "#call vectorizer on text columns\n",
    "tfidf_numbers=tfidf_vectorizer.fit_transform(df['tfidfprep']) \n",
    "\n",
    "#convert vectorizer data to dataframe\n",
    "df1 = pd.DataFrame(tfidf_numbers.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "print(\"df1 SHAPE = \"+str(df1.shape))\n",
    "print(df1.columns)\n",
    "#sources is the one hot encoding of the parsedURL\n",
    "extensions = pd.get_dummies(df['extension'])\n",
    "print(\"EXTENSIONS SHAPE = \"+str(extensions.shape))\n",
    "print(extensions.columns)\n",
    "\n",
    "#drop duplicates?\n",
    "#df1.drop_duplicates(inplace=True)\n",
    "#sources.drop_duplicates(inplace=True)\n",
    "\n",
    "#join vectorizer data and one hot encoded sources columns\n",
    "#numberDF = pd.concat([df1, sources], axis = 1)\n",
    "numberDF = extensions.join(df1, lsuffix = '_left', rsuffix = '_right')\n",
    "\n",
    "\n",
    "labels=df[[\"true/false\"]]\n",
    "\n",
    "#create train and test sets\n",
    "x_train, x_test, y_train, y_test= train_test_split(numberDF, labels, test_size=0.2, stratify=df['true/false'], random_state=7)\n",
    "\n",
    "print(numberDF.head)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.49      0.65        87\n",
      "        True       0.73      0.98      0.84       125\n",
      "\n",
      "    accuracy                           0.78       212\n",
      "   macro avg       0.83      0.74      0.74       212\n",
      "weighted avg       0.82      0.78      0.76       212\n",
      "\n",
      "[[ 43  44]\n",
      " [  3 122]]\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0)\n",
    "\n",
    "lr.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "lr_y_pred = lr.predict(x_test)\n",
    "\n",
    "\n",
    "print(classification_report(y_test,lr_y_pred))\n",
    "print(confusion_matrix(y_test,lr_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.67      0.77      0.72        87\n",
      "        True       0.82      0.74      0.78       125\n",
      "\n",
      "    accuracy                           0.75       212\n",
      "   macro avg       0.75      0.75      0.75       212\n",
      "weighted avg       0.76      0.75      0.75       212\n",
      "\n",
      "[[67 20]\n",
      " [33 92]]\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "dt.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "dt_y_pred = dt.predict(x_test)\n",
    "dt_features = dt.feature_importances_\n",
    "\n",
    "print(classification_report(y_test,dt_y_pred))\n",
    "print(confusion_matrix(y_test,dt_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.79      0.64      0.71        87\n",
      "        True       0.78      0.88      0.83       125\n",
      "\n",
      "    accuracy                           0.78       212\n",
      "   macro avg       0.78      0.76      0.77       212\n",
      "weighted avg       0.78      0.78      0.78       212\n",
      "\n",
      "[[ 56  31]\n",
      " [ 15 110]]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "rf.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "rf_y_pred = rf.predict(x_test)\n",
    "rf_features = rf.feature_importances_\n",
    "\n",
    "print(classification_report(y_test,rf_y_pred))\n",
    "print(confusion_matrix(y_test,rf_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Aggresive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      0.91      0.84        87\n",
      "        True       0.93      0.82      0.87       125\n",
      "\n",
      "    accuracy                           0.85       212\n",
      "   macro avg       0.85      0.86      0.85       212\n",
      "weighted avg       0.86      0.85      0.85       212\n",
      "\n",
      "[[ 79   8]\n",
      " [ 23 102]]\n"
     ]
    }
   ],
   "source": [
    "#instantiate passive aggresive classifier\n",
    "pac=PassiveAggressiveClassifier(max_iter=100, random_state=0)\n",
    "\n",
    "#fit classifer to tfidf of training set\n",
    "pac.fit(x_train,y_train.values.ravel())\n",
    "\n",
    "#create predictions on test set\n",
    "pac_y_pred = pac.predict(x_test)\n",
    "\n",
    "#print score\n",
    "print(classification_report(y_test,pac_y_pred))\n",
    "print(confusion_matrix(y_test,pac_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

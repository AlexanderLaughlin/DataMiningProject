{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact or Fake: News Analysis\n",
    "\n",
    "## Data Mining 334\n",
    "## Alex Laughlin, Xandre Clementsmith, Terence Carey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Creation and Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in data\n",
    "\n",
    "#read in politifact\n",
    "df_p_real = pd.read_csv('politifact_real.csv')\n",
    "df_p_fake = pd.read_csv('politifact_fake.csv')\n",
    "\n",
    "df_p_real['true/false'] = True\n",
    "\n",
    "df_p_fake['true/false'] = False\n",
    "\n",
    "#read in gossipcop data\n",
    "df_g_real = pd.read_csv('gossipcop_real.csv')\n",
    "df_g_fake = pd.read_csv('gossipcop_fake.csv')\n",
    "\n",
    "df_g_real['true/false'] = True\n",
    "\n",
    "df_g_fake['true/false'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            news_url  \\\n",
      "0                          http://www.nfib-sbet.org/   \n",
      "1  http://www.cq.com/doc/newsmakertranscripts-494...   \n",
      "2  https://web.archive.org/web/20080204072132/htt...   \n",
      "3  https://web.archive.org/web/20110811143753/htt...   \n",
      "4  https://web.archive.org/web/20070820164107/htt...   \n",
      "\n",
      "                                               title  true/false  \n",
      "0        National Federation of Independent Business        True  \n",
      "1                        comments in Fayetteville NC        True  \n",
      "2  Romney makes pitch, hoping to close deal : Ele...        True  \n",
      "3  Democratic Leaders Say House Democrats Are Uni...        True  \n",
      "4    Budget of the United States Government, FY 2008        True  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1056, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Join tables together\n",
    "sets = \"p\"\n",
    "if sets == \"p\":\n",
    "    frames = [df_p_real, df_p_fake]\n",
    "elif sets == \"g\":\n",
    "    frames = [df_g_real, df_g_fake]\n",
    "elif sets == \"b\":\n",
    "    frames = [df_p_real, df_p_fake, df_g_real, df_g_fake]\n",
    "df = pd.concat(frames)\n",
    "df = df[['news_url', 'title','true/false']]\n",
    "print(df.head(5))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     624\n",
       "False    432\n",
       "Name: true/false, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many values of each truth value (label)\n",
    "df['true/false'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "True_False_count=df.groupby('true/false').count()\n",
    "plt.bar(True_False_count.index.values, True_False_count['title'])\n",
    "plt.xlabel('Truth')\n",
    "plt.ylabel('Number of Titles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipelinize(function, active=True):\n",
    "    def list_comprehend_a_function(list_or_series,active=True):\n",
    "        if active:\n",
    "            return [function(i) for i in list_or_series]\n",
    "        else:\n",
    "            return list_or_series\n",
    "    return FunctionTransformer(list_comprehend_a_function,\n",
    "                              validate=False, kw_args={'active':active})\n",
    "\n",
    "def tokenize(text):\n",
    "\n",
    "    tokenizer = nltk.RegexpTokenizer(r\"\\w{3,}\")\n",
    "    \n",
    "    \n",
    "    #remove special characters\n",
    "    #data['title'] = data.apply(lambda row: re.sub('[^A-Za-z ]{3,}', '', str(row['title'])), axis =1)\n",
    "    title = re.sub('[^A-Za-z ]{3,}', '', text)\n",
    "    \n",
    "    #tokenize words from title\n",
    "    tokenized_sents = tokenizer.tokenize(title)\n",
    "    #tokenized_sents = title.apply(lambda row: tokenizer.tokenize(row['title']))\n",
    "    \n",
    "    #remove stop words from tokenized titles\n",
    "    tokens_without_stopwords = [item for item in tokenized_sents if item not in stop_words]\n",
    "    #tokens_without_stopwords = tokenized_sents.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "    \n",
    "    return tokens_without_stopwords\n",
    "    \n",
    "    \n",
    "\n",
    "def stem(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    #stem tokenized words without stopwords\n",
    "    #tokens_stemmed=tokens_without_stopwords.apply(lambda x : [stemmer.stem(y) for y in x])\n",
    "    tokens_stemmed=[stemmer.stem(y) for y in text]\n",
    "    return tokens_stemmed\n",
    "        \n",
    "def lemmatize(text):\n",
    "    \n",
    "    #lemmatize tokenized words without stopwords\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    tokens_lemmatized = [lemmatizer.lemmatize(y) for y in text]\n",
    "    \n",
    "    return tokens_lemmatized\n",
    "\n",
    "#parse source URL and strip down to base form \n",
    "from urllib.parse import urlparse\n",
    "import urllib as urllib\n",
    "df['parsedURL'] = df.apply(lambda row: urlparse(str(row['news_url'])).netloc, axis=1)\n",
    "df['extension'] = df.apply(lambda row: str(row['parsedURL'])[-3:], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "stemEstimators = [('tokenizer', pipelinize(tokenize)), ('stemmer', pipelinize(stem))]\n",
    "lemEstimators = [('tokenizer', pipelinize(tokenize)), ('lemmatizer', pipelinize(lemmatize))]\n",
    "\n",
    "stemPipe = Pipeline(stemEstimators)\n",
    "lemPipe = Pipeline(lemEstimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_url</th>\n",
       "      <th>title</th>\n",
       "      <th>true/false</th>\n",
       "      <th>parsedURL</th>\n",
       "      <th>extension</th>\n",
       "      <th>tokens_stemmed</th>\n",
       "      <th>tokens_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.nfib-sbet.org/</td>\n",
       "      <td>National Federation of Independent Business</td>\n",
       "      <td>True</td>\n",
       "      <td>www.nfib-sbet.org</td>\n",
       "      <td>org</td>\n",
       "      <td>[nation, feder, independ, busi]</td>\n",
       "      <td>[National, Federation, Independent, Business]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.cq.com/doc/newsmakertranscripts-494...</td>\n",
       "      <td>comments in Fayetteville NC</td>\n",
       "      <td>True</td>\n",
       "      <td>www.cq.com</td>\n",
       "      <td>com</td>\n",
       "      <td>[comment, fayettevill]</td>\n",
       "      <td>[comment, Fayetteville]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://web.archive.org/web/20080204072132/htt...</td>\n",
       "      <td>Romney makes pitch, hoping to close deal : Ele...</td>\n",
       "      <td>True</td>\n",
       "      <td>web.archive.org</td>\n",
       "      <td>org</td>\n",
       "      <td>[romney, make, pitch, hope, close, deal, elect...</td>\n",
       "      <td>[Romney, make, pitch, hoping, close, deal, Ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://web.archive.org/web/20110811143753/htt...</td>\n",
       "      <td>Democratic Leaders Say House Democrats Are Uni...</td>\n",
       "      <td>True</td>\n",
       "      <td>web.archive.org</td>\n",
       "      <td>org</td>\n",
       "      <td>[democrat, leader, say, hous, democrat, are, u...</td>\n",
       "      <td>[Democratic, Leaders, Say, House, Democrats, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://web.archive.org/web/20070820164107/htt...</td>\n",
       "      <td>Budget of the United States Government, FY 2008</td>\n",
       "      <td>True</td>\n",
       "      <td>web.archive.org</td>\n",
       "      <td>org</td>\n",
       "      <td>[budget, unit, state, govern]</td>\n",
       "      <td>[Budget, United, States, Government]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.politifact.com/truth-o-meter/statem...</td>\n",
       "      <td>Donald Trump exaggerates when he says China ha...</td>\n",
       "      <td>True</td>\n",
       "      <td>www.politifact.com</td>\n",
       "      <td>com</td>\n",
       "      <td>[donald, trump, exagger, say, china, total, co...</td>\n",
       "      <td>[Donald, Trump, exaggerates, say, China, total...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.law.cornell.edu/constitution/amend...</td>\n",
       "      <td>25th Amendment</td>\n",
       "      <td>True</td>\n",
       "      <td>www.law.cornell.edu</td>\n",
       "      <td>edu</td>\n",
       "      <td>[25th, amend]</td>\n",
       "      <td>[25th, Amendment]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://americaneedsmitt.com/blog/2011/11/10/mi...</td>\n",
       "      <td>子供たちのコト。私のコト。</td>\n",
       "      <td>True</td>\n",
       "      <td>americaneedsmitt.com</td>\n",
       "      <td>com</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://web.archive.org/web/20090913221204/htt...</td>\n",
       "      <td>Briefing by White House Press Secretary Robert...</td>\n",
       "      <td>True</td>\n",
       "      <td>web.archive.org</td>\n",
       "      <td>org</td>\n",
       "      <td>[brief, white, hous, press, secretari, robert,...</td>\n",
       "      <td>[Briefing, White, House, Press, Secretary, Rob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://web.archive.org/web/20080623122709/htt...</td>\n",
       "      <td>A Solar Grand Plan: Scientific American</td>\n",
       "      <td>True</td>\n",
       "      <td>web.archive.org</td>\n",
       "      <td>org</td>\n",
       "      <td>[solar, grand, plan, scientif, american]</td>\n",
       "      <td>[Solar, Grand, Plan, Scientific, American]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            news_url  \\\n",
       "0                          http://www.nfib-sbet.org/   \n",
       "1  http://www.cq.com/doc/newsmakertranscripts-494...   \n",
       "2  https://web.archive.org/web/20080204072132/htt...   \n",
       "3  https://web.archive.org/web/20110811143753/htt...   \n",
       "4  https://web.archive.org/web/20070820164107/htt...   \n",
       "5  http://www.politifact.com/truth-o-meter/statem...   \n",
       "6  https://www.law.cornell.edu/constitution/amend...   \n",
       "7  http://americaneedsmitt.com/blog/2011/11/10/mi...   \n",
       "8  https://web.archive.org/web/20090913221204/htt...   \n",
       "9  https://web.archive.org/web/20080623122709/htt...   \n",
       "\n",
       "                                               title  true/false  \\\n",
       "0        National Federation of Independent Business        True   \n",
       "1                        comments in Fayetteville NC        True   \n",
       "2  Romney makes pitch, hoping to close deal : Ele...        True   \n",
       "3  Democratic Leaders Say House Democrats Are Uni...        True   \n",
       "4    Budget of the United States Government, FY 2008        True   \n",
       "5  Donald Trump exaggerates when he says China ha...        True   \n",
       "6                                     25th Amendment        True   \n",
       "7                                      子供たちのコト。私のコト。        True   \n",
       "8  Briefing by White House Press Secretary Robert...        True   \n",
       "9            A Solar Grand Plan: Scientific American        True   \n",
       "\n",
       "              parsedURL extension  \\\n",
       "0     www.nfib-sbet.org       org   \n",
       "1            www.cq.com       com   \n",
       "2       web.archive.org       org   \n",
       "3       web.archive.org       org   \n",
       "4       web.archive.org       org   \n",
       "5    www.politifact.com       com   \n",
       "6   www.law.cornell.edu       edu   \n",
       "7  americaneedsmitt.com       com   \n",
       "8       web.archive.org       org   \n",
       "9       web.archive.org       org   \n",
       "\n",
       "                                      tokens_stemmed  \\\n",
       "0                    [nation, feder, independ, busi]   \n",
       "1                             [comment, fayettevill]   \n",
       "2  [romney, make, pitch, hope, close, deal, elect...   \n",
       "3  [democrat, leader, say, hous, democrat, are, u...   \n",
       "4                      [budget, unit, state, govern]   \n",
       "5  [donald, trump, exagger, say, china, total, co...   \n",
       "6                                      [25th, amend]   \n",
       "7                                                 []   \n",
       "8  [brief, white, hous, press, secretari, robert,...   \n",
       "9           [solar, grand, plan, scientif, american]   \n",
       "\n",
       "                                   tokens_lemmatized  \n",
       "0      [National, Federation, Independent, Business]  \n",
       "1                            [comment, Fayetteville]  \n",
       "2  [Romney, make, pitch, hoping, close, deal, Ele...  \n",
       "3  [Democratic, Leaders, Say, House, Democrats, A...  \n",
       "4               [Budget, United, States, Government]  \n",
       "5  [Donald, Trump, exaggerates, say, China, total...  \n",
       "6                                  [25th, Amendment]  \n",
       "7                                                 []  \n",
       "8  [Briefing, White, House, Press, Secretary, Rob...  \n",
       "9         [Solar, Grand, Plan, Scientific, American]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens_stemmed'] = stemPipe.transform(df['title'])\n",
    "df['tokens_lemmatized'] = lemPipe.transform(df['title'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODULAR COMPONENT:\n",
    "# select token_type for machine learning algorithms\n",
    "# tokens_stemmed performs best\n",
    "token_type = 'tokens_stemmed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create corpus from lemmatized tokens as numpy array\n",
    "corpus = df[token_type].to_numpy()\n",
    "\n",
    "#create dictionary\n",
    "DF = {}\n",
    "\n",
    "#write unique words to dictionary\n",
    "for i in range(len(corpus)):\n",
    "    tokens = corpus[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "            \n",
    "#get word frequency\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])\n",
    "# print(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 SHAPE = (1056, 2517)\n",
      "Index(['10k', '22m', '25th', '27ave', '2kilo', '2nd', '3rd', '3year',\n",
      "       'abandon', 'abc',\n",
      "       ...\n",
      "       'wwii', 'year', 'yearbook', 'yellow', 'york', 'young', 'younger',\n",
      "       'youngest', 'youtub', 'zakaria'],\n",
      "      dtype='object', length=2517)\n",
      "EXTENSIONS SHAPE = (1056, 23)\n",
      "Index(['', '.be', '.co', '.is', '.me', '.pw', '.ru', '.tk', '.uk', '.us',\n",
      "       'com', 'edu', 'ews', 'gov', 'ife', 'ite', 'ive', 'lub', 'mil', 'net',\n",
      "       'nfo', 'one', 'org'],\n",
      "      dtype='object')\n",
      "<bound method NDFrame.head of         .be  .co  .is  .me  .pw  .ru  .tk  .uk  .us  ...  wwii  year  \\\n",
      "0    0    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "0    1    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "1    0    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "1    1    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "2    0    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "..  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   \n",
      "619  0    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "620  0    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "621  0    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "622  0    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "623  0    0    0    0    0    0    0    0    0    0  ...   0.0   0.0   \n",
      "\n",
      "     yearbook  yellow  york  young  younger  youngest  youtub  zakaria  \n",
      "0         0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "0         0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "1         0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "1         0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "2         0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "..        ...     ...   ...    ...      ...       ...     ...      ...  \n",
      "619       0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "620       0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "621       0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "622       0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "623       0.0     0.0   0.0    0.0      0.0       0.0     0.0      0.0  \n",
      "\n",
      "[1056 rows x 2540 columns]>\n"
     ]
    }
   ],
   "source": [
    "#create column of strings from lemmatized tokens\n",
    "df['tfidfprep']=[\" \".join(x) for x in df[token_type].values]\n",
    "\n",
    "#initialize tfidf vectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "#call vectorizer on text columns\n",
    "tfidf_numbers=tfidf_vectorizer.fit_transform(df['tfidfprep']) \n",
    "\n",
    "#convert vectorizer data to dataframe\n",
    "df1 = pd.DataFrame(tfidf_numbers.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "print(\"df1 SHAPE = \"+str(df1.shape))\n",
    "print(df1.columns)\n",
    "#sources is the one hot encoding of the parsedURL\n",
    "extensions = pd.get_dummies(df['extension'])\n",
    "print(\"EXTENSIONS SHAPE = \"+str(extensions.shape))\n",
    "print(extensions.columns)\n",
    "\n",
    "#drop duplicates?\n",
    "#df1.drop_duplicates(inplace=True)\n",
    "#sources.drop_duplicates(inplace=True)\n",
    "\n",
    "#join vectorizer data and one hot encoded sources columns\n",
    "#numberDF = pd.concat([df1, sources], axis = 1)\n",
    "numberDF = extensions.join(df1, lsuffix = '_left', rsuffix = '_right')\n",
    "\n",
    "\n",
    "labels=df[[\"true/false\"]]\n",
    "\n",
    "#create train and test sets\n",
    "x_train, x_test, y_train, y_test= train_test_split(numberDF, labels, test_size=0.2, stratify=df['true/false'], random_state=7)\n",
    "\n",
    "print(numberDF.head)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.66      0.77       345\n",
      "        True       0.80      0.97      0.88       499\n",
      "\n",
      "    accuracy                           0.84       844\n",
      "   macro avg       0.87      0.81      0.83       844\n",
      "weighted avg       0.86      0.84      0.84       844\n",
      "\n",
      "[[226 119]\n",
      " [ 13 486]]\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.49      0.65        87\n",
      "        True       0.73      0.98      0.84       125\n",
      "\n",
      "    accuracy                           0.78       212\n",
      "   macro avg       0.83      0.74      0.74       212\n",
      "weighted avg       0.82      0.78      0.76       212\n",
      "\n",
      "[[ 43  44]\n",
      " [  3 122]]\n"
     ]
    }
   ],
   "source": [
    "lr = Pipeline([('classifier', LogisticRegression(random_state=0))])\n",
    "\n",
    "lr.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "print(\"Training Data\")\n",
    "lr_y_pred = lr.predict(x_train)\n",
    "#print score\n",
    "print(classification_report(y_train,lr_y_pred))\n",
    "print(confusion_matrix(y_train,lr_y_pred))\n",
    "\n",
    "lr_y_pred = lr.predict(x_test)\n",
    "\n",
    "print(\"Test Data\")\n",
    "print(classification_report(y_test,lr_y_pred))\n",
    "print(confusion_matrix(y_test,lr_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.91      1.00      0.95       345\n",
      "        True       1.00      0.93      0.96       499\n",
      "\n",
      "    accuracy                           0.96       844\n",
      "   macro avg       0.96      0.97      0.96       844\n",
      "weighted avg       0.96      0.96      0.96       844\n",
      "\n",
      "[[345   0]\n",
      " [ 34 465]]\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.67      0.77      0.72        87\n",
      "        True       0.82      0.74      0.78       125\n",
      "\n",
      "    accuracy                           0.75       212\n",
      "   macro avg       0.75      0.75      0.75       212\n",
      "weighted avg       0.76      0.75      0.75       212\n",
      "\n",
      "[[67 20]\n",
      " [33 92]]\n"
     ]
    }
   ],
   "source": [
    "dt = Pipeline([('classifier', DecisionTreeClassifier(random_state=0))])\n",
    "\n",
    "dt.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "print(\"Training Data\")\n",
    "dt_y_pred = dt.predict(x_train)\n",
    "#print score\n",
    "print(classification_report(y_train,dt_y_pred))\n",
    "print(confusion_matrix(y_train,dt_y_pred))\n",
    "\n",
    "dt_y_pred = dt.predict(x_test)\n",
    "\n",
    "print(\"Test Data\")\n",
    "print(classification_report(y_test,dt_y_pred))\n",
    "print(confusion_matrix(y_test,dt_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.95      0.95       345\n",
      "        True       0.97      0.96      0.96       499\n",
      "\n",
      "    accuracy                           0.96       844\n",
      "   macro avg       0.96      0.96      0.96       844\n",
      "weighted avg       0.96      0.96      0.96       844\n",
      "\n",
      "[[329  16]\n",
      " [ 20 479]]\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.62      0.71        87\n",
      "        True       0.77      0.90      0.83       125\n",
      "\n",
      "    accuracy                           0.79       212\n",
      "   macro avg       0.80      0.76      0.77       212\n",
      "weighted avg       0.79      0.79      0.78       212\n",
      "\n",
      "[[ 54  33]\n",
      " [ 12 113]]\n"
     ]
    }
   ],
   "source": [
    "rf = Pipeline([('classifier', RandomForestClassifier(random_state=0, min_samples_split = 5))])\n",
    "\n",
    "rf.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "print(\"Training Data\")\n",
    "rf_y_pred = rf.predict(x_train)\n",
    "#print score\n",
    "print(classification_report(y_train,rf_y_pred))\n",
    "print(confusion_matrix(y_train,rf_y_pred))\n",
    "\n",
    "rf_y_pred = rf.predict(x_test)\n",
    "\n",
    "print(\"Test Data\")\n",
    "print(classification_report(y_test,rf_y_pred))\n",
    "print(confusion_matrix(y_test,rf_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Aggresive Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.89      0.97      0.93       345\n",
      "        True       0.98      0.92      0.95       499\n",
      "\n",
      "    accuracy                           0.94       844\n",
      "   macro avg       0.94      0.95      0.94       844\n",
      "weighted avg       0.95      0.94      0.94       844\n",
      "\n",
      "[[336   9]\n",
      " [ 40 459]]\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.77      0.91      0.84        87\n",
      "        True       0.93      0.82      0.87       125\n",
      "\n",
      "    accuracy                           0.85       212\n",
      "   macro avg       0.85      0.86      0.85       212\n",
      "weighted avg       0.86      0.85      0.85       212\n",
      "\n",
      "[[ 79   8]\n",
      " [ 23 102]]\n"
     ]
    }
   ],
   "source": [
    "#instantiate passive aggresive classifier\n",
    "pac = Pipeline([('classifier', PassiveAggressiveClassifier(max_iter=100, random_state=0))])\n",
    "\n",
    "\n",
    "pac.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "print(\"Training Data\")\n",
    "pac_y_pred = pac.predict(x_train)\n",
    "#print score\n",
    "print(classification_report(y_train,pac_y_pred))\n",
    "print(confusion_matrix(y_train,pac_y_pred))\n",
    "\n",
    "pac_y_pred = pac.predict(x_test)\n",
    "\n",
    "print(\"Test Data\")\n",
    "print(classification_report(y_test,pac_y_pred))\n",
    "print(confusion_matrix(y_test,pac_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Aggressive Classifier with lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 SHAPE = (1056, 3016)\n",
      "Index(['10k', '22m', '25th', '27ave', '2kilos', '2nd', '3rd', '3year',\n",
      "       'abandon', 'abc',\n",
      "       ...\n",
      "       'year', 'yearbook', 'years', 'yellow', 'york', 'young', 'younger',\n",
      "       'youngest', 'youtube', 'zakaria'],\n",
      "      dtype='object', length=3016)\n",
      "EXTENSIONS SHAPE = (1056, 23)\n",
      "Index(['', '.be', '.co', '.is', '.me', '.pw', '.ru', '.tk', '.uk', '.us',\n",
      "       'com', 'edu', 'ews', 'gov', 'ife', 'ite', 'ive', 'lub', 'mil', 'net',\n",
      "       'nfo', 'one', 'org'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#create column of strings from lemmatized tokens\n",
    "df['tfidfprep']=[\" \".join(x) for x in df['tokens_lemmatized'].values]\n",
    "labels=df[[\"true/false\"]]\n",
    "\n",
    "#initialize tfidf vectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "#call vectorizer on text columns\n",
    "tfidf_numbers=tfidf_vectorizer.fit_transform(df['tfidfprep']) \n",
    "\n",
    "#convert vectorizer data to dataframe\n",
    "df1 = pd.DataFrame(tfidf_numbers.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "print(\"df1 SHAPE = \"+str(df1.shape))\n",
    "print(df1.columns)\n",
    "#sources is the one hot encoding of the parsedURL\n",
    "extensions = pd.get_dummies(df['extension'])\n",
    "print(\"EXTENSIONS SHAPE = \"+str(extensions.shape))\n",
    "print(extensions.columns)\n",
    "\n",
    "#drop duplicates?\n",
    "#df1.drop_duplicates(inplace=True)\n",
    "#sources.drop_duplicates(inplace=True)\n",
    "\n",
    "#join vectorizer data and one hot encoded sources columns\n",
    "#numberDF = pd.concat([df1, sources], axis = 1)\n",
    "numberDF = extensions.join(df1, lsuffix = '_left', rsuffix = '_right')\n",
    "\n",
    "\n",
    "labels=df[[\"true/false\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.98      0.93       345\n",
      "        True       0.98      0.91      0.94       499\n",
      "\n",
      "    accuracy                           0.94       844\n",
      "   macro avg       0.93      0.94      0.93       844\n",
      "weighted avg       0.94      0.94      0.94       844\n",
      "\n",
      "[[338   7]\n",
      " [ 47 452]]\n",
      "Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.73      0.91      0.81        87\n",
      "        True       0.92      0.77      0.84       125\n",
      "\n",
      "    accuracy                           0.83       212\n",
      "   macro avg       0.83      0.84      0.82       212\n",
      "weighted avg       0.84      0.83      0.83       212\n",
      "\n",
      "[[79  8]\n",
      " [29 96]]\n"
     ]
    }
   ],
   "source": [
    "pac.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "print(\"Training Data\")\n",
    "pac_y_pred = pac.predict(x_train)\n",
    "#print score\n",
    "print(classification_report(y_train,pac_y_pred))\n",
    "print(confusion_matrix(y_train,pac_y_pred))\n",
    "\n",
    "pac_y_pred = pac.predict(x_test)\n",
    "\n",
    "print(\"Test Data\")\n",
    "print(classification_report(y_test,pac_y_pred))\n",
    "print(confusion_matrix(y_test,pac_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The Passive Aggressor classifier has the highest accuracy at 85%. It additionally also has the best precision and recall scores, so we consider it to be the best model for prediction of the validity of headlines. We also compare the accuracy of the PAC and stemmed words with the accuracy of the PAC and lemmatized words and see that stemmed wins by 2%. For precision and recall as well, stemmed words are better than lemmatized by 2%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
